지금까지 훈련 속도를 높이는 네 가지 방법을 보았다.  



1. 연결 가중치에 좋은 초기화 전략 적용하기

2. 좋은 활성화 함수 사용하기

3. 배치 정규화 사용하기

4. 사전훈련된 네트워크의 일부 재사용하기  



이번 포스트에서는 표준적인 경사 하강법 옵티마이저 대신 훈련 속도를 크게 높일 수 있는 또 다른 방법인 고속 옵티마이저에 대해 다뤄볼 것이다.

 

## 11.3 고속 옵티마이저

훈련 속도를 크게 높일 수 있는 또 다른 방법으로는 표준적인 경사 하강법 옵티마이저 대신 더 빠른 옵티마이저를 사용하는 것이다.

1. 모멘텀 최적화

2. 네스테로프 가속 경사

3. AdaGrad

4. RMSProp

5. Adam

6. Nadam





### 11.3.1 모멘텀 최적화

- 경사하강법  

  - **방식**:그레디언트가 속도로 사용되는 방법이다.

  - (갱신 가중치) <- (현재 가중치) - (학습률) * (가중치에 대한 비용함수의 그레디언트)  

      ![image](https://user-images.githubusercontent.com/89712324/220684195-93b0d1f5-0311-43df-88ec-53dc6d9dbeec.png)

  - 이전 그레디언트가 얼마였는지 고려하지 않는다.

  - 그레디언트가 아주 작으면 매우 느려지는 특징이 있다.



- 모멘텀 최적화

  - **방식**: 그레디언트가 속도가 아닌 가속도로 사용된 방법이다. 그렇기에 가속도 클 수록 더 빠르게 최적에 수렴할 수 있고 완만한 지역도 속도가 줄지 않고 빠르게 수렴할 수 있다.  

  - 모멘텀 벡터 m에 이전 그레디언트에 대한 정보가 포함되어있다.  

      ![image](https://user-images.githubusercontent.com/89712324/220685183-28aa2d4f-d155-4385-8c73-6286a966e749.png) 

  - 모멘텀 m이 너무 커지는 것을 방지하기 위해서 모멘텀 β가 주어지며, β는 가속에 대한 공기 저항 부분에 해당된다.

  - 모멘텀이 0.9이면, 종단 속도는 η x 그레디언트 x {1-/(1-β)} 이므로 경사하강법보다 10배 빨라진다. 즉 β가 클 수록 공기 저항이 줄어든다.

  - 지역 최적점을 건너뛰도록 하는데 도움이 된다.

    - 모멘텀 때문에 옵티마이저가 최적값에 안정되기 전까지 건너뛰었다가 다시 돌아오고, 다시 또 건너 뛰는 방식으로 여러 번 왔다 갔다할 수 있다. 여기서 마찰 저항은 이런 진동을 없애주고 빠르게 수렴할 수 있도록 한다.  

  - 실제로 모멘텀 0.9에서 잘 작동하며 경사 하강법보다 거의 항상 더 빠르다.



**케라스로 구현하기**

- learning_rate => η (학습률)

- momentum => β (모멘텀, 값이 클 수록 공기 저항이 줄어들며 모멘텀이 유지된다)



```python
import keras
optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
```

### 11.3.2 네스테로프 가속 경사 (NAG)

- 모멘텀 최적화의 변종이고, 기본 모멘텀 최적화보다 거의 항상 더 빠르다.

- **방식**: 현재 위치가 θ가 아니라 모멘텀의 방향으로 조금 앞선 θ+βxm 에서 비용함수의 그레디언트를 계산한다.  

  ![image](https://user-images.githubusercontent.com/89712324/220838203-9ff62db8-7eee-4198-a498-2e3f9c4cd344.png)

- 일반적으로 모멘텀 벡터가 올바른 방향을 가리키기 때문에 원래 위치보다는 앞 단계의 그레디언트를 사용하는 것이 조금 더 정확하다.  


**케라스로 구현하기**



```python
import keras
optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)
```

### 11.3.3 AdaGrad

- 전역 최적점 방향에 따라 그레디언트 벡터의 스케일을 감소시킨다.   

  1. 각 차원<sub>i</sub>에 해당하는 그레디언트<sub>i</sub>의 제곱을 벡터s<sub>i</sub> 에 누적한다. 즉 가파를 수록 s<sub>i</sub>는 크게 커진다.

  2. 그레디언트 벡터를 √{s_i+ε}로 나누어 스케일 조정한다. (ε는 0으로 나누는 것을 막기 위한 값)  

    ![image](https://user-images.githubusercontent.com/89712324/220853928-08d20e32-ac5c-4db9-8b4d-204e13df34f8.png)  

- (**적응적 학습률**): 학습률을 감소시키지만 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소된다.

- 학습률 하이퍼파라미터η를 덜 튜닝해도 된다.  

  ![image](https://user-images.githubusercontent.com/89712324/220873247-dad73d65-ced1-43f8-ba47-fb78d30f11ae.png)  

- **단점**: 학습률이 너무 감소되어 전역 최적점에 도달하기 전에 알고리즘이 완전히 멈추는 경우가 있다. 따라서 선형 회귀 같은 간단한 작업에는 효과적이지만, 심층 심경망에서는 사용하지 말아야한다!


### 11.3.4 RMSProp

- 모든 그레디언트가 아닌 가장 최근 반복에서 비롯된 그레디언트에 더 높은 가중치를 부여하는 방식으로 AdaGrad의 문제를 해결할 수 있다.  

  ![image](https://user-images.githubusercontent.com/89712324/220933051-29a1204a-58a5-472c-8931-d23045366326.png)

  - 1번에서 가장 최근 반복에서 비롯된 그레디언트에 더 높은 가중치를 부여하기 위해 지수 감소를 사용하였다.

  - 보통 감쇠율β는 0.9로 설정하며, 기본값이 잘 작동하는 경우가 많아 튜닝할 필요는 없다.  

- 아주 간단한 문제를 제외하고는 RMSprop 옵티마이저가 AdaGrad보다 훨씬 성능이 좋다. Adam 최적화가 나오기 전까지 가장 선호하는 최적화 알고리즘이었다.



**케라스로 구현하기**  




```python
import keras
optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

### 11.3.5 Adam과 Nadam 최적화

- Adam: 모멘텀 최적화와 RMSProp의 아이디어를 합친 것이다.

  - 모멘텀 최적화: 지난 그레디언트의 지수 감소 평균을 따르고 

  - RMSProp: 지난 그레디언트 제곱의 지수 감소된 평균을 따른다.



  ![image](https://user-images.githubusercontent.com/89712324/221097327-d103c072-9fbb-4d99-90bf-19683462f6f4.png)  

  ![image](https://user-images.githubusercontent.com/89712324/221097454-f545654d-f447-4bf6-9c39-09c360c5b6d8.png)



- 1, 2, 5 단계 => 모멘텀 최적화와 RMSProp와 매우 비슷하다.

- 1단계 => 지수 감소 합 대신 지수 감소 평균을 계산한다.

- 3, 4 단계 => m과 s가 0으로 초기화 되므로 훈련 초기에 값을 m과 s값을 증폭시킨다.

- 모멘텀 감쇠 하이퍼파라미터β_1는 보통 0.9로 초기화하고, 스케일 감쇠 하이퍼파라미터β_2는 0.999로 초기화한다.  



**케라스로 코드 구현**



```python
import keras
optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
```

**Adam의 변종 2가지 소개**

**AdaMax**  

- Adam의 L\_2 norm을 L_∞ norm으로 바꾼다.

- Adam보다 더 안정적이다.

**Nadam**

- Adam 옵티마이저에 네스테로프 기법을 더한 것이다. 그렇기에 Adam보다 조금 더 빠르게 수렴한다.

- Nadam이 Adam보다 성능이 좋지만 RMSProp가 나을 때도 있다.  


### 지금까지 소개한 옵티마이저 비교하기

![image](https://user-images.githubusercontent.com/89712324/221417828-e9e87a83-d656-428e-b64b-135621ef3da9.png)
