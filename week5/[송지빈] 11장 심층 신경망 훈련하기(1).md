# [Hands-On ML] 11장 심층 신경망 훈련하기(1)
"핸즈온 머신러닝"의 "11장 심층 신경망 훈련하기" 파트를 읽고 공부한 내용을 정리해보았다. 그레디언트 소실과 ReLU 활성화 함수에 대해 다뤄볼 것이다.<br><br>

# 11장 심층 신경망 훈련하기



심층 신경망 훈련 문제에 대한 해결방법을 알아보자<br>

1. 그레이디언트 소실, 폭주 문제

2. 레이블된 훈련 데이터가 적을 때 => 전이 학습 & 비지도 학습으로 해결

3. 대규모 모델의 훈련 속도 증가시키는 최적화 방법

4. 대규모 신경망 과대적합을 막기 위한 규제 기법


<br>

<br>


# 11.1 그레이디언트 소실과 폭주 문제

- 역전파 알고리즘: 출력층에서 입력층으로 오차 그레디언트를 전파하면서 진행된다. 각 층의 모든 그레디언트를 구하면 경사 하강법 단계에서 그레디언트를 이용해서 각 파라미터를 수정한다.

- 그레이디언트 소실이란: 알고리즘이 하위층으로 진행될 수록 그레이디언트가 점점 작아져 0으로 수렴되는 현상이다.

- 그레이디언트 폭주란: 그레이디언트가 점점 커져서 여러층이 비정상적으로 큰 가중치로 갱신되어 발산하는 현상이다. RNN에서 주로 나타난다. 

- 그레디언트가 불안정해지면 층마다 학습 속도가 달라지고 잘 훈련되지 않는다.



- 그레디언트가 불안정해지는 이유

  1. 로지스틱 시그모이드 활성화 함수

  2. 가중치 초기화 방법(표준정규분포)<br>

![image](https://user-images.githubusercontent.com/89712324/216768608-f87c5bb6-84eb-44ed-9f1d-49ccd21aec6a.png)

  - 층을 지날 때마다 분산이 커져 가장 높은 층에서는 활성화 함수가 0이나 1로 수렴하게 된다.

  - 로지스틱 출력의 평균이 0이 아니라 0.5이므로 출력의 가중치 합이 입력보다 커지는 편향 이동이 발생한다. 로지스틱 입력이 커지면 0이나 1로 수렴하여 기울기가 0에 가까워진다. 따라서 역전파가 될 때 사실상 신경망으로 전파할 그레디언트가 0으로 수렴한다.


<br>

## 11.1.1 글로럿과 He 초기화

글로럿과 벤지오가 불안정한 그레디언트 문제 완화하는 방법을 제시했다.

1. 예측을 할 때는 정방향으로, 그레디언트를 역전파할 때는 역방향으로 신호가 흐를 때, 중간에 신호가 죽거나 폭주 또는 소멸하지 않아야한다.

2. 각 층의 출력에 대한 분산이 입력의 분산과 같아야한다.

3. 역방향에서 층을 통과하기 전후의 그레디언트 분산이 같아야한다.


<br>


### 글로럿 초기화(세이비어 초기화)

각 층의 연결 가중치를 아래와 같이 무작위로 초기화 하기

<br> ![image](https://user-images.githubusercontent.com/89712324/216769817-ac23b625-2e85-4945-ac9f-1d09a35bbe4e.png)

- 글로럿 초기화 방법을 사용하면 그레디언트가 폭주하거나 소멸하지 않아 훈련 속도를 상당히 높아질 수 있다.



<br>

### He 초기화

글로럿 초기화 전략에 활성화 함수를 ReLU로한 초기화 방법이다.<br>

케라스는 균등분포의 글로럿 초기화를 사용한다. He 초기화 방법을 사용하고 싶으면 아래 코드를 작성하면 된다.<br>

![image](https://user-images.githubusercontent.com/89712324/216770177-0639e3b6-2b9b-4350-85ed-172b31836086.png)



```python
keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')

# 또는 분산을 바꾸고 싶으면 아래와 같이 작성한다.
he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',
                                                 distribution='uniform')
keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)
```

<br>

## 11.1.2 수렴하지 않는 활성화 함수

활성화 함수를 잘못 선택하면 그레디언트의 소실이나 폭주로 이어질 수 있다.  

ReLU 활성화 함수를 선택하자!

<br>

### ReLU

- ReLU 장점

  - ReLU 함수는 입력값이 커져도 특정 값에 수렴하지 않는다. 또한 계산도 빠르다.  

- ReLU 단점: 죽은 ReLU  

  - 훈련하는 동안 일부 뉴런이 0의 값만 출력하는 현상이다. 

  - 특히 큰 학습률을 사용하면 뉴런 절반이 죽는다. 

  - 모든 샘플에 대해 입력의 가중치 합이 음수가 나오면 ReLU 함수의 그레디언트가 0이 되므로 경사하강법이 작동하지 않는다. (손실에 영향을 미치는 정도가 계속 0이 나오기 때문!)

<br>



### LeakyReLU

LeakyReLU = max(az, z) <br>

![image](https://user-images.githubusercontent.com/89712324/216771030-2b41fb58-8b49-487b-8524-9edd35def9e2.png)  

- 하이퍼파라미터 a가 z가 음수일 때, 함수의 기울기이다. 일반적으로 a = 0.01이지만 a = 0.2로 하는 것이 더 나은 성능을 낸다고 저자는 말했다. (교차검증 해보자) 즉 뉴런이 혼수 상태에 오래 있을 수 있지만 다시 깨어날 가능성을 얻게 된다.

- ReLU보다 항상 성능이 높게 나온다는 논문 결과가 있다.

<br>



### RReLU (Randomized Leaky ReLU)

- a를 무작위로 선택하고 테스트시에 평균을 사용한다.

- 괜찮은 결과가 나오며, 규제 역할도 할 수 있다.



<br>

### PReLU (Parametric Leaky ReLU)

- a 자체를 학습시킨다.

- 대규모 이미지셋에서는 ReLU보다 성능이 크게 앞섰지만, 소규모 데이터셋에서는 훈련세트에 과대적합될 위험이 있다.


<br>


### ELU (Exponential Linear Unit)

![image](https://user-images.githubusercontent.com/89712324/216771103-bdc4c608-76a4-4d58-9248-9ec784e11003.png)

- 다른 모든 ReLU 변종의 성능을 앞질렀다.

- 훈련 시간이 줄고 신경망의 테스트 세트 성능도 더 높았다.

- ELU 장점

  - 활성화 함수의 평균 출력이 0에 가까워진다. 그레디언트 소실 문제를 완화해준다.

  - z가 음수여도 그레디언트가 0이 아니므로 죽은 뉴런을 만들지 않는다.

  - a가 1일 때, z = 0에서 기울기가 1로 모든 구간이 매끄러우므로 경사하강법의 속도를 높여준다.

- ELU 단점

  - 훈련하는 동안에 최적에 수렴 속도가 빠르지만 테스트 시에는 ReLU보다 속도가 느리다.


<br>


### SELU (Scaled ELU)

- 완전 연결 층만 쌓아서 신경망을 만들고 모든 은닉층이 SELU 함수를 사용하면 자기 정규화 된다 => 훈련하는 동안 평균 0과 표준편차 1를 유지하는 경향이 있다.

- 자기 정규화 발생 조건

  - 입력 특성이 반드시 표준화(평균0, 표준편차1)되어야한다.

  - 모든 은닉층의 가중치는 르쿤 정규분포 초기화로 초기화 되어야한다. kernel_initializer='lecun_normal'

  - 네트워크가 일렬로 쌓여야한다.


<br>


### TIP

- 일반적인 은닉층 활성화 함수 사용 순위  

  SELU > ELU > LeakyReLU(그리고 변종들) > LeLU > tanh > 로지스틱

- 자기 정규화되지 못하는 구조 => ELU

- 실행 속도 중요 => LeakyReLU

- 신경망이 과대적합되었다면 => RReLU

- 훈련세트가 아주 크다면 PReLU

- 속도가 중요하다면 => ReLU  

  (많은 라이브러리와 하드웨어 가속기들이 ReLU에 특화되어 최적화되어있기 때문)




<br>



```python
# LeakyReLU 활성화 함수
model = keras.models.Sequential([
    # [...]
    keras.layers.Dense(10, kernel_initialize='he_normal'),
    keras.layers.LeakyReLU(alpha=0.2),
    # [...]
])
```


```python
# PReLU 활성화 함수
model = keras.models.Sequential([
    # [...]
    keras.layers.Dense(10, kernel_initialize='he_normal'),
    keras.layers.PReLU(),
    # [...]
])
```


```python
# SELU 활성화 함수
keras.layers.Dense(10, activation='selu', kernel_initialize='lecun_normal')
```
<br>

지금까지 그레디언트 소실과 ReLU 활성화 함수 그리고 ReLU의 변종 함수들에 대해 알아보았다. 다음 포스트에서는 배치 정규화와 그레디언트 클램핑에 대해 다뤄볼 것이다.
